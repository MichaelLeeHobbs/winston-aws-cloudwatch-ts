---
name: deep-review
description: Comprehensive multi-agent code review with security, standards, architecture, YAGNI, and goal analysis
disable-model-invocation: true
arguments:
  - name: scope
    description: "Scope to review: a directory path, file glob, or 'all' for the entire src/ directory. Defaults to 'all'."
    required: false
---

# Deep Code Review â€” Multi-Agent Analysis

You are orchestrating a comprehensive code review using 5 specialized reviewer agents running in parallel. This produces a detailed review document with severity-tagged findings.

## Scope

The review scope is: **$ARGUMENTS** (default: `all` â€” review the entire `src/` directory)

## Instructions

### 1. Prepare the review

- Generate a timestamp in ISO format (e.g., `2024-01-15T14:30:00`)
- Determine files to review:
  - If scope is `all` or empty: all `.ts` files under `src/`
  - If scope is a directory: all `.ts` files under that directory
  - If scope is a glob: matching files
- Read the reference checklists:
  - `.claude/skills/deep-review/references/review-checklist.md`
  - `.claude/skills/deep-review/references/security-checklist.md`
- Check if the coding standard exists at `documents/TypeScript Coding Standard for Mission-Critical Systems.md` and load it if present

### 2. Launch 5 parallel reviewer agents

Use the **Task tool** to launch these 5 agents in parallel. Each agent should receive:
- The list of files to review
- The relevant checklist/standard sections
- Instructions to output findings in the structured format below

Read the agent definitions from `.claude/agents/` for each reviewer's specific instructions:

#### Agent 1: Security Reviewer
- **Agent definition**: `.claude/agents/security-reviewer.md`
- **Focus**: OWASP Top 10, input validation (Rules 7.2, 7.4), secrets, injection, crypto

#### Agent 2: Standards Reviewer
- **Agent definition**: `.claude/agents/standards-reviewer.md`
- **Focus**: All shall-level rules from the coding standard

#### Agent 3: YAGNI Reviewer
- **Agent definition**: `.claude/agents/yagni-reviewer.md`
- **Focus**: Over-engineering, dead code, unnecessary abstractions, premature optimization

#### Agent 4: Architecture Reviewer
- **Agent definition**: `.claude/agents/architecture-reviewer.md`
- **Focus**: Modularity (Rule 10.3), dependency direction, SOLID principles, circular deps

#### Agent 5: Goal Reviewer
- **Agent definition**: `.claude/agents/goal-reviewer.md`
- **Focus**: Does the code achieve its stated goals? Edge cases? Completeness?

### 3. Consolidate findings

After all 5 agents complete, merge their findings into a single review document.

Each finding should be tagged with a severity:
- ðŸ”´ **CRITICAL** â€” Must fix. Security vulnerability, data loss risk, or standard violation that could cause runtime failure
- ðŸŸ  **HIGH** â€” Should fix. Significant standard violation or architectural concern
- ðŸŸ¡ **MEDIUM** â€” Consider fixing. Minor standard deviation or improvement opportunity
- ðŸ”µ **LOW** â€” Nice to have. Style suggestion or minor optimization

De-duplicate findings that appear in multiple reviewers' output (keep the highest severity).

### 4. Write the review document

Create the directory `docs/reviews/` if it doesn't exist, then write the consolidated review to `docs/reviews/review-${TIMESTAMP}.md`:

```markdown
# Deep Code Review â€” ${TIMESTAMP}

**Scope**: ${SCOPE}
**Files reviewed**: N
**Reviewers**: Security, Standards, YAGNI, Architecture, Goal

## Executive Summary

<2-4 sentences summarizing overall code quality and top concerns>

**Findings**: N total (X critical, Y high, Z medium, W low)

## Critical Findings

- [ ] ðŸ”´ `file.ts:42` â€” **[Security]** Description
- [ ] ðŸ”´ `file.ts:15` â€” **[Standards]** Description

## High Findings

- [ ] ðŸŸ  `file.ts:88` â€” **[Architecture]** Description

## Medium Findings

- [ ] ðŸŸ¡ `file.ts:22` â€” **[YAGNI]** Description

## Low Findings

- [ ] ðŸ”µ `file.ts:5` â€” **[Goal]** Description

## Per-Reviewer Details

### Security Review
<Security reviewer's detailed findings>

### Standards Compliance Review
<Standards reviewer's detailed findings>

### YAGNI Review
<YAGNI reviewer's detailed findings>

### Architecture Review
<Architecture reviewer's detailed findings>

### Goal Alignment Review
<Goal reviewer's detailed findings>

## Recommendations

1. Prioritized list of actions
2. ...

---
*Generated by /deep-review on ${TIMESTAMP}*
```

### 5. Report to user

- Show the file path of the generated review
- Display the executive summary and critical findings count
- If there are critical findings, emphasize they must be addressed before deployment
